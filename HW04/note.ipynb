{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[[-1.0901,  1.1819,  0.7886,  1.5535, -1.2427,  1.7660,  0.9284,\n",
      "          -0.7842,  0.0192, -0.6306, -0.9772,  0.6619,  1.2552,  1.8749,\n",
      "          -0.3533, -0.7700,  1.4728,  1.4153, -0.8875, -0.8448,  1.2200,\n",
      "           1.1723,  1.0875, -1.4292, -0.0329,  0.4312, -1.1617,  0.6495,\n",
      "           0.3001, -0.3996, -0.1957, -0.5547, -1.2100, -0.2962,  0.2951,\n",
      "           0.1526, -0.9361,  0.4924,  0.0291, -1.5464],\n",
      "         [ 0.7884, -0.7737, -1.1455,  1.2430, -0.1077,  2.3955, -1.0904,\n",
      "          -0.5161,  1.4699, -0.7851,  0.5706, -1.1925, -1.9294,  1.0363,\n",
      "           0.5553, -0.7292, -1.8897, -0.4809, -0.6555, -1.4076, -0.5577,\n",
      "           0.1653,  0.8924,  0.9308, -0.7766,  0.8844, -1.7648,  0.9387,\n",
      "          -0.3380,  1.9968,  0.5960, -2.3631, -0.1019,  1.1034,  1.3185,\n",
      "          -1.0731,  0.8066, -0.9332,  1.2089,  0.3577]]])\n",
      "output_tensor.shape torch.Size([1, 2, 80])\n"
     ]
    }
   ],
   "source": [
    "# 不属于本作业的代码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(40, 80)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('x', x)\n",
    "        out = self.fc1(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "x = torch.randn(1, 2, 40)\n",
    "\n",
    "module = Classifier()\n",
    "output_tensor = module(x)\n",
    "print('output_tensor.shape', output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "53\n",
      "tensor([[0.6433, 0.8647, 0.9158],\n",
      "        [0.8107, 0.0056, 0.4446],\n",
      "        [0.6455, 0.7661, 0.2941]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "torch.FloatTensor([speaker]) tensor([44.])\n",
      "torch.FloatTensor([speaker]).long() tensor([44])\n"
     ]
    }
   ],
   "source": [
    "# 不属于该作业的代码\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# 创建一个形状为(3, 4)的torch.FloatTensor对象，数据类型为float32\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]], dtype=torch.float32)\n",
    "\n",
    "mel = 555\n",
    "segment_len = 128\n",
    "start = random.randint(0, mel - segment_len)\n",
    "\n",
    "print(len(x))\n",
    "print(x[1:3])\n",
    "print(start)\n",
    "\n",
    "b = torch.rand(3,3)#得到的是floattensor值，\n",
    "print(b)\n",
    "b = b.long()#得到的是longtensor值\n",
    "print(b)\n",
    "\n",
    "print('torch.FloatTensor([speaker])', torch.FloatTensor([44]))\n",
    "print('torch.FloatTensor([speaker]).long()', torch.FloatTensor([44]).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "a.shape torch.Size([2, 3])\n",
      "b.shape torch.Size([1, 3])\n",
      "c.shape torch.Size([3, 3])\n",
      "torch.Size([3, 3, 3])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [5., 5., 5.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [5., 5., 5.],\n",
      "         [5., 5., 5.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 不属于该作业的代码\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 最后一维必须一致，可以理解为 embedding 层\n",
    "a = torch.ones(2, 3)\n",
    "b = torch.ones(1, 3)\n",
    "c = torch.ones(3, 3)\n",
    "\n",
    "print('c', c)\n",
    "print('a.shape', a.shape)\n",
    "print('b.shape', b.shape)\n",
    "print('c.shape', c.shape)\n",
    "\n",
    "print(pad_sequence((a, b, c), batch_first=False, padding_value=20).size()) # 如果是()，就是一个元组，里面的 a，b，c 都不是 list\n",
    "print(pad_sequence([a, b, c], batch_first=False, padding_value=20).size())\n",
    "\n",
    "# batch_first=True 表示将批次维度放在序列的第一维，便于后续处理。\n",
    "print(pad_sequence([a, b, c], batch_first=True, padding_value=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e.shape torch.Size([3])\n",
      "f.shape torch.Size([2])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5, 20]])\n"
     ]
    }
   ],
   "source": [
    "e = torch.tensor([1, 2, 3])\n",
    "f = torch.tensor([4, 5])\n",
    "\n",
    "print('e.shape', e.shape)\n",
    "print('f.shape', f.shape)\n",
    "print(pad_sequence([e, f], batch_first=True, padding_value=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 pad_sequence: https://blog.csdn.net/Zhao_knight/article/details/120913610\n",
    "\n",
    "<font color=red> 如果出现报错，是由于 矩阵是由数字组成的矩阵，也就是基本单元都是数字，但是，再高一层次(既是+2 也是-2 个维度)的数据类型是list，而不是数组。</font>\n",
    "\n",
    "解决：使用 torch.tensor() 对第二层的数据类型做一个类型转换，不适用list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 0., 0.],\n",
      "        [4., 5., 6., 7., 8.],\n",
      "        [8., 9., 0., 0., 0.]])\n",
      "tensor([[1., 2., 3., 0., 0.],\n",
      "        [4., 5., 6., 7., 8.],\n",
      "        [8., 9., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "input_x =[[1,2,3],[4,5,6,7,8],[8,9]]\n",
    "norm_data_pad = pad_sequence([torch.from_numpy(np.array(x)) for x in input_x], batch_first=True).float()\n",
    "print(norm_data_pad)\n",
    "\n",
    "input_x = [torch.tensor([1,2,3]),torch.tensor([4,5,6,7,8]),torch.tensor([8,9])]\n",
    "norm_data_pad = pad_sequence(input_x, batch_first=True).float()\n",
    "print(norm_data_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collate_fn\n",
    "\n",
    "pytorch中collate_fn函数的使用&如何向collate_fn函数传参: https://blog.csdn.net/dong_liuqi/article/details/114521240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argparse 库\n",
    "\n",
    "Python：argparse 库高级用法举例和应用详解：https://evzs.com/2024/07/25/Python%EF%BC%9Aargparse%E5%BA%93%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95%E4%B8%BE%E4%BE%8B%E5%92%8C%E5%BA%94%E7%94%A8%E8%AF%A6%E8%A7%A3.html\n",
    "\n",
    "argparse --- 用于命令行选项、参数和子命令的解析器：https://docs.python.org/zh-cn/3.12/library/argparse.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你没有提供名字.\n"
     ]
    }
   ],
   "source": [
    "# 不属于该作业的代码\n",
    "import argparse  # 导入argparse库\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='示例 1: 基本用法')  # 创建ArgumentParser对象并添加描述\n",
    "    # 前面没有'-'，属于位置参数。\n",
    "    # 前面有'-'，属于可选参数。\n",
    "    parser.add_argument('--name', help='你的名字')  # 添加一个名为name的可选参数\n",
    "\n",
    "    # 调用parser.parse_args()会读取系统参数：sys.argv[]，命令行调用时是正确参数，而在jupyter notebook中调用时，sys.argv的值为ipykrnel_launcher.py：因此会报错\n",
    "    # args = parser.parse_args()  # 解析命令行参数\n",
    "\n",
    "    args = parser.parse_known_args()[0] # 解析命令行参数\n",
    "    if args.name:  # 如果name参数存在，打印欢迎消息\n",
    "        print(f\"你好, {args.name}!\")\n",
    "    else:  # 如果name参数不存在，提示参数缺失\n",
    "        print(\"你没有提供名字.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# 运行上面的脚本: python script.py --name Alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t---> tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "index---> tensor([1, 2])\n",
      "data1---> tensor([[[ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "data2---> tensor([[[ 1,  2],\n",
      "         [ 5,  6],\n",
      "         [ 9, 10]],\n",
      "\n",
      "        [[13, 14],\n",
      "         [17, 18],\n",
      "         [21, 22]]])\n"
     ]
    }
   ],
   "source": [
    "# 不属于该作业的代码\n",
    "# index_select 函数\n",
    "t = torch.arange(24).reshape(2, 3, 4) # 初始化一个tensor，从0到23，形状为（2,3,4）\n",
    "print(\"t--->\", t)\n",
    " \n",
    "index = torch.tensor([1, 2]) # 要选取数据的位置\n",
    "print(\"index--->\", index)\n",
    "\n",
    "# .reshape(2, 3, 4) 可以看出 2 是第1维，3 是第2维，4是第3维\n",
    "data1 = t.index_select(1, index) # 第一个参数:从第2维挑选， 第二个参数:从该维中挑选的位置\n",
    "print(\"data1--->\", data1)\n",
    " \n",
    "data2 = t.index_select(2, index) # 第一个参数:从第3维挑选， 第二个参数:从该维中挑选的位置\n",
    "print(\"data2--->\", data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_indices.flatten():\n",
      " tensor([0, 1, 2, 2, 3, 4])\n",
      "\n",
      "Embedding Matrix:\n",
      " Parameter containing:\n",
      "tensor([[ 0.2906, -0.2304, -0.2201],\n",
      "        [ 2.3460,  1.2444,  2.0486],\n",
      "        [ 0.8249, -2.1700,  0.5731],\n",
      "        [-0.4615,  0.4468, -0.6046],\n",
      "        [ 1.4969,  0.2865, -0.7065]], requires_grad=True)\n",
      "\n",
      "Input Indices:\n",
      " tensor([[0, 1, 2],\n",
      "        [2, 3, 4]])\n",
      "\n",
      "input_indices.size:\n",
      " torch.Size([2, 3])\n",
      "\n",
      "input_indices.size(0):\n",
      " 2\n",
      "\n",
      "Embedded Vectors:\n",
      " tensor([[[ 0.2906, -0.2304, -0.2201],\n",
      "         [ 2.3460,  1.2444,  2.0486],\n",
      "         [ 0.8249, -2.1700,  0.5731]],\n",
      "\n",
      "        [[ 0.8249, -2.1700,  0.5731],\n",
      "         [-0.4615,  0.4468, -0.6046],\n",
      "         [ 1.4969,  0.2865, -0.7065]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 不属于该作业的代码\n",
    "# transformer\n",
    "import torch\n",
    "# 初始化词嵌入矩阵\n",
    "vocabulary_size = 5\n",
    "embedding_dim = 3\n",
    "embedding_matrix = torch.nn.Parameter(torch.randn(vocabulary_size, embedding_dim))\n",
    "# 输入索引值\n",
    "input_indices = torch.LongTensor([[0, 1, 2], [2, 3, 4]])\n",
    "# 根据输入索引值查找词嵌入向量\n",
    "# index_select：https://blog.csdn.net/kdongyi/article/details/103099589\n",
    "# 第一个参数：dim：表示从第几维挑选数据，类型为int值；第二个参数：index：表示从第一个参数维度中的哪个位置挑选数据\n",
    "embedded_vectors = torch.index_select(embedding_matrix, dim=0, index=input_indices.flatten())\n",
    "# 将词嵌入向量按照输入索引顺序进行拼接\n",
    "# .view 相当于 numpy中resize（）的功能，但是用法可能不太一样。\n",
    "embedded_vectors = embedded_vectors.view(input_indices.size(0), input_indices.size(1), embedding_dim) # size 为 2, 3, 3\n",
    "\n",
    "print(\"input_indices.flatten():\\n\", input_indices.flatten())\n",
    "print('')\n",
    "\n",
    "print(\"Embedding Matrix:\\n\", embedding_matrix)\n",
    "print('')\n",
    "\n",
    "print(\"Input Indices:\\n\", input_indices)\n",
    "print('')\n",
    "\n",
    "print(\"input_indices.size:\\n\", input_indices.size())\n",
    "print('')\n",
    "\n",
    "print(\"input_indices.size(0):\\n\", input_indices.size(0))\n",
    "print('')\n",
    "\n",
    "print(\"Embedded Vectors:\\n\", embedded_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
